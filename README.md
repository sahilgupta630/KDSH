# ğŸ“š Backstory Consistency NLP - KDSH '26

> **A Retrieval Augmented Generation (RAG) system for verifying character backstory consistency in literature.**

<div align="center">

![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![Groq](https://img.shields.io/badge/Groq-API-orange?style=for-the-badge)
![HuggingFace](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-yellow?style=for-the-badge)
![Pathway](https://img.shields.io/badge/Pathway-Data%20Processing-blue?style=for-the-badge)
![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=for-the-badge&logo=pandas&logoColor=white)

</div>

## ğŸ† Project Overview

This project was developed as a solution for the **Kharagpur Data Science Hackathon (KDSH) 2026** Challenge.

The goal of this system is to automatically validate the consistency of character backstories against a provided corpus of books. By leveraging advanced NLP techniques, the system decomposes complex backstories into atomic claims, retrieves relevant evidence from the source texts, and uses a Large Language Model (LLM) to verify whether the claims are supported, contradicted, or not mentioned in the text.

## ğŸ› ï¸ Tech Stack

This project is built using a modern AI/NLP stack:

*   **ğŸ Python**: The core programming language.
*   **ğŸš… Pathway**: A high-performance data processing framework used for efficient document ingestion and handling live data streams.
*   **ğŸ§  SentenceTransformers**: Used for generating semantic embeddings (`all-MiniLM-L6-v2`) to capture the meaning of text chunks.
*   **âš¡ Groq API**: Powers the high-speed inference for the LLM (Llama 3 / Mixtral) used in reasoning and verification.
*   **ğŸ” Cross-Encoder**: Utilized for precise re-ranking of retrieved evidence to ensure the most relevant context is sent to the LLM.
*   **ğŸ¼ Pandas**: Employed for structured data manipulation and results tracking.

## ğŸ“‚ Project Structure

| File | Description |
| :--- | :--- |
| `Code.ipynb` | ğŸ““ Original Jupyter Notebook used for initial development and experiments. |
| `data_ingestion.py` | ğŸ“¥ Handles loading books and creating vector embeddings using **Pathway**. |
| `query_generator.py` | ğŸ§© Uses an LLM to decompose backstories into verifiable atomic claims and search queries. |
| `evidence_retrieval.py` | ğŸ” Retrieves and re-ranks evidence chunks from the book corpus. |
| `verification.py` | âš–ï¸ The "Judge" - verifies claims against retrieved evidence using the LLM. |
| `validator.py` | âš™ï¸ Orchestrates the entire pipeline, processing the dataset and generating `results.csv`. |
| `Dataset/` | ğŸ“ Contains `train.csv`, `test.csv`, and the `Books/` directory. |

## ğŸš€ Setup & Usage

### Prerequisites
- Python 3.10+
- A Groq API Key

### Installation

1.  **Clone the repository**
    ```bash
    git clone https://github.com/sahilgupta630/KDSH.git
    cd KDSH
    ```

2.  **Install dependencies**
    ```bash
    pip install -r requirements.txt
    ```

### Running the Pipeline

You can run the full validation pipeline using the `validator.py` script (or by importing it).

```python
import os
from groq import Groq
from data_ingestion import NovelIngestionPipeline
from validator import run_validation

# 1. Setup Client
client = Groq(api_key="YOUR_GROQ_API_KEY")

# 2. Ingest Data (Books)
pipeline = NovelIngestionPipeline("Dataset/Books")
index_df = pipeline.run_indexing()

# 3. Run Validation
results = run_validation(client, "llama3-70b-8192", index_df, limit=10)
print(results)
```

## ğŸ“„ License
This project is open-source and available under the MIT License.
